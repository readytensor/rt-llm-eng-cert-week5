{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb480a6",
   "metadata": {},
   "source": [
    "# Load Llama 3.2 1B, Apply LoRA, and Push Adapters to Hugging Face\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load the Llama 3.2 1B Instruct model from Hugging Face\n",
    "2. Apply LoRA adapters (without training)\n",
    "3. Push the LoRA adapters to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df1eda",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Make sure you have the required packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5985a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if needed\n",
    "# !pip install transformers accelerate torch huggingface_hub peft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f5744",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "449bef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dff636",
   "metadata": {},
   "source": [
    "## Authenticate with Hugging Face\n",
    "\n",
    "You need to be logged in to Hugging Face to:\n",
    "1. Access gated models like Llama 3.2\n",
    "2. Push models to your account\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ce0a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face - you'll be prompted for your token\n",
    "# Make sure you have accepted the Llama 3.2 license on Hugging Face\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_USERNAME = os.environ.get(\"HF_USERNAME\")\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72397f6f",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3587b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Your Hugging Face username and desired repo name\n",
    "if HF_USERNAME is None:\n",
    "    HF_USERNAME = input(\"Enter your Hugging Face username: \")\n",
    "\n",
    "if HF_TOKEN is None:\n",
    "    HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "NEW_MODEL_NAME = \"llama-3.2-1b-lora-adapters\"  # <-- Change this to your desired model name\n",
    "\n",
    "# Full repo ID for pushing\n",
    "PUSH_REPO_ID = f\"{HF_USERNAME}/{NEW_MODEL_NAME}\"\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_R = 16  # Rank of the LoRA matrices\n",
    "LORA_ALPHA = 32  # Alpha scaling factor\n",
    "LORA_DROPOUT = 0.05  # Dropout probability\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c18f4",
   "metadata": {},
   "source": [
    "## Load the Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "121fea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Model loaded successfully!\n",
      "Model has 1,235,814,400 parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load model\n",
    "# Using float16 to save memory, adjust based on your hardware\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Automatically distribute across available devices\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0451196",
   "metadata": {},
   "source": [
    "## Test the Original Model (Optional)\n",
    "\n",
    "Let's generate some text before modifying the weights to see how it behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82a09954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ORIGINAL MODEL OUTPUT:\n",
      "==================================================\n",
      "What is the meaning of life? A question that has puzzled philosophers, theologians, scientists, and everyday people for centuries. The answer, of course, is subjective and can vary greatly depending on one's beliefs, values, and experiences.\n",
      "\n",
      "Some people believe that the meaning of life is to find happiness, fulfillment, and purpose. They may think that the key to a happy life is to pursue one's passions, build meaningful relationships, and cultivate a sense of gratitude and contentment.\n",
      "\n",
      "Others may believe that the meaning of life is to\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.01,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"What is the meaning of life?\"\n",
    "print(\"=\" * 50)\n",
    "print(\"ORIGINAL MODEL OUTPUT:\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_text(model, tokenizer, test_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd9894",
   "metadata": {},
   "source": [
    "## Apply LoRA Adapters\n",
    "\n",
    "We'll apply LoRA (Low-Rank Adaptation) adapters to the model without training. The adapters will be initialized with random weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2dc87477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Configuration:\n",
      "  - Rank (r): 16\n",
      "  - Alpha: 32\n",
      "  - Dropout: 0.05\n",
      "  - Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  - Rank (r): {LORA_R}\")\n",
    "print(f\"  - Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  - Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  - Target modules: {TARGET_MODULES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed317293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapters to the model...\n",
      "\n",
      "LoRA adapters applied successfully!\n",
      "Trainable parameters: 3,407,872\n",
      "Total parameters: 1,239,222,272\n",
      "Trainable %: 0.28%\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to the model\n",
    "print(\"Applying LoRA adapters to the model...\")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nLoRA adapters applied successfully!\")\n",
    "print(f\"Trainable parameters: {peft_model.num_parameters(only_trainable=True):,}\")\n",
    "print(f\"Total parameters: {peft_model.num_parameters():,}\")\n",
    "print(f\"Trainable %: {100 * peft_model.num_parameters(only_trainable=True) / peft_model.num_parameters():.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2035a70",
   "metadata": {},
   "source": [
    "## View LoRA Adapter Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9adf8c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter modules:\n",
      "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n",
      "\n",
      "LoRA layers added:\n",
      "  - base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 16])\n",
      "  - base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: torch.Size([512, 16])\n",
      "  - base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: torch.Size([16, 2048])\n",
      "  - base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: torch.Size([2048, 16])\n"
     ]
    }
   ],
   "source": [
    "# Show LoRA adapter modules\n",
    "print(\"LoRA adapter modules:\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA layers added:\")\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(f\"  - {name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c104030",
   "metadata": {},
   "source": [
    "## Test the Model with LoRA (Optional)\n",
    "\n",
    "Let's see how the model behaves with LoRA adapters. Since we haven't trained the adapters, the output should be similar to the base model (LoRA A matrices are initialized to random values and B matrices to zero, so initially the adapters have minimal effect).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "033a08fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MODEL WITH LORA OUTPUT:\n",
      "==================================================\n",
      "What is the meaning of life? A question that has puzzled philosophers, theologians, scientists, and everyday people for centuries. The answer, of course, is subjective and can vary greatly depending on one's beliefs, values, and experiences.\n",
      "\n",
      "Some people believe that the meaning of life is to find happiness, fulfillment, and purpose. They may think that the key to a happy life is to pursue one's passions, build meaningful relationships, and cultivate a sense of gratitude and contentment.\n",
      "\n",
      "Others may believe that the meaning of life is to\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"MODEL WITH LORA OUTPUT:\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_text(peft_model, tokenizer, test_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8615110",
   "metadata": {},
   "source": [
    "## Push the LoRA Adapters to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8e4bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing LoRA adapters to: moo3030/llama-3.2-1b-lora-adapters\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 13.6MB / 13.6MB,  316kB/s  \n",
      "New Data Upload: 100%|██████████| 13.6MB / 13.6MB,  316kB/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters pushed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pushing LoRA adapters to: {PUSH_REPO_ID}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Push only the LoRA adapters (not the full model)\n",
    "peft_model.push_to_hub(\n",
    "    PUSH_REPO_ID,\n",
    "    commit_message=\"Upload LoRA adapters for Llama 3.2 1B\",\n",
    "    private=True  # Set to False if you want a public repo\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters pushed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cb7c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 17.2MB / 17.2MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pushed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Push the tokenizer as well\n",
    "tokenizer.push_to_hub(\n",
    "    PUSH_REPO_ID,\n",
    "    commit_message=\"Upload tokenizer for Llama 3.2 1B LoRA\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizer pushed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee8249",
   "metadata": {},
   "source": [
    "## Create Model Card (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData\n",
    "\n",
    "# Calculate trainable params for the card\n",
    "trainable_params = peft_model.num_parameters(only_trainable=True)\n",
    "total_params = peft_model.num_parameters()\n",
    "\n",
    "# Create a model card\n",
    "card_data = ModelCardData(\n",
    "    language=\"en\",\n",
    "    license=\"llama3.2\",\n",
    "    base_model=MODEL_ID,\n",
    "    tags=[\"llama\", \"lora\", \"peft\", \"adapter\"]\n",
    ")\n",
    "\n",
    "card_content = f\"\"\"\n",
    "# {NEW_MODEL_NAME}\n",
    "\n",
    "This is a LoRA adapter for [{MODEL_ID}](https://huggingface.co/{MODEL_ID}).\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model:** {MODEL_ID}\n",
    "- **Adapter Type:** LoRA (Low-Rank Adaptation)\n",
    "- **LoRA Rank (r):** {LORA_R}\n",
    "- **LoRA Alpha:** {LORA_ALPHA}\n",
    "- **LoRA Dropout:** {LORA_DROPOUT}\n",
    "- **Target Modules:** {', '.join(TARGET_MODULES)}\n",
    "- **Trainable Parameters:** {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\n",
    "\n",
    "## Note\n",
    "\n",
    "⚠️ These LoRA adapters have NOT been trained - they contain only their random initialization.\n",
    "This is intended for experimental/educational purposes.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_ID}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_ID}\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, \"{PUSH_REPO_ID}\")\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This adapter inherits the Llama 3.2 Community License from the base model.\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(card_content)\n",
    "card.push_to_hub(PUSH_REPO_ID)\n",
    "\n",
    "print(\"Model card created and pushed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e20ef",
   "metadata": {},
   "source": [
    "## Load and Test the Pushed Model\n",
    "\n",
    "Now let's verify that the pushed adapters work by loading them fresh from Hugging Face and generating predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5860769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from: moo3030/llama-3.2-1b-lora-adapters\n",
      "--------------------------------------------------\n",
      "LoRA adapters loaded successfully from Hugging Face Hub!\n",
      "Model parameters: 1,239,222,272\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# First, clear the existing model from memory to simulate a fresh load\n",
    "del peft_model\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"Loading LoRA adapters from: {PUSH_REPO_ID}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load a fresh base model\n",
    "base_model_fresh = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the LoRA adapters from Hugging Face Hub\n",
    "loaded_model = PeftModel.from_pretrained(base_model_fresh, PUSH_REPO_ID)\n",
    "\n",
    "print(\"LoRA adapters loaded successfully from Hugging Face Hub!\")\n",
    "print(f\"Model parameters: {loaded_model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8f4531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADED MODEL PREDICTIONS:\n",
      "==================================================\n",
      "\n",
      "Prompt: What is the meaning of life?\n",
      "----------------------------------------\n",
      "Response: A question that has puzzled philosophers, theologians, scientists, and everyday people for centuries. The answer, of course, is subjective and can vary greatly depending on one's beliefs, values, and experiences.\n",
      "\n",
      "Some people believe that the meaning of life is to find happiness, fulfillment, and purpose. They may think that the key to a happy life is to pursue one's passions, build meaningful relationships,\n",
      "\n",
      "\n",
      "Prompt: Explain quantum computing in simple terms.\n",
      "----------------------------------------\n",
      "Response: Imagine you have a huge library with an infinite number of books, each representing a possible solution to a problem. In classical computing, you would have to look through the entire library one book at a time to find the solution. But with quantum computing, you can use the principles of quantum mechanics to quickly and efficiently search through the library.\n",
      "\n",
      "Here's a simplified example:\n",
      "\n",
      "**Classical Computing:**\n",
      "\n",
      "Imagine\n",
      "\n",
      "\n",
      "Prompt: Write a haiku about programming.\n",
      "----------------------------------------\n",
      "Response: Code is like a puzzle, and the solution is the answer.\n",
      "\n",
      "In the dark of the screen\n",
      "Puzzle pieces fit together\n",
      "Code is the answer\n",
      "\n",
      "Note: Traditional haiku typically consist of three lines with a syllable count of 5-7-5. I've followed this structure in the haiku above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions with the loaded model\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADED MODEL PREDICTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about programming.\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    output = generate_text(loaded_model, tokenizer, prompt, max_new_tokens=80)\n",
    "    # Remove the prompt from the output for cleaner display\n",
    "    response = output[len(prompt):].strip() if output.startswith(prompt) else output\n",
    "    print(f\"Response: {response}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58b3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
